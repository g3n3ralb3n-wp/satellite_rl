{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Satellite Sensor Tasking with Reinforcement Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand reinforcement learning fundamentals (states, actions, rewards, Q-learning)\n",
    "2. Apply RL to satellite sensor tasking problem\n",
    "3. Visualize agent learning and convergence\n",
    "4. Understand the impact of sparse rewards on learning\n",
    "5. Connect grid-based learning to physical satellite gimbal pointing\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "Satellites with sensors (cameras, radar) need to decide where to point to observe ground targets.\n",
    "This notebook models the problem as a grid where the agent learns to navigate to high-value target locations.\n",
    "\n",
    "**Setup**: Ensure you're using the satellite_rl kernel (see README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "from environment.grid_env import SatelliteSensorGridEnv\n",
    "from agents.q_learning import QLearningAgent\n",
    "from visualization.grid_viz import plot_learning_curve, plot_value_heatmap, plot_policy_arrows\n",
    "from visualization.satellite_viz import create_satellite_gimbal_visualization, grid_to_satellite_coordinates\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Matplotlib settings\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## Step 1: Create the Environment\n",
    "\n",
    "We'll create an 11x11 grid environment where:\n",
    "- The satellite sensor can \"point\" to any of 121 positions\n",
    "- The goal (high-value target) is at the center position (state 60)\n",
    "- Rewards are given ONLY for reaching positions directly adjacent to the goal\n",
    "- Actions: 0=up, 1=down, 2=left, 3=right\n",
    "\n",
    "**Important**: This is a **sparse reward** environment - only 4 out of 121 states give rewards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_env",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SatelliteSensorGridEnv(grid_x=11, grid_y=11)\n",
    "\n",
    "print(f\"Environment created!\")\n",
    "print(f\"  Grid size: {env.grid_x}x{env.grid_y}\")\n",
    "print(f\"  Total states: {env.observation_space.n}\")\n",
    "print(f\"  Actions: {env.action_space.n} (0=up, 1=down, 2=left, 3=right)\")\n",
    "print(f\"  Goal state: {env.goal_state}\")\n",
    "print(f\"  Goal position: (row={env.goal_state // 11}, col={env.goal_state % 11})\")\n",
    "\n",
    "# Visualize initial random state\n",
    "state, _ = env.reset()\n",
    "print(f\"\\nInitial random state: {state}\")\n",
    "print(f\"\\nNote: Only 4 states (adjacent to goal) give +100 reward.\")\n",
    "print(f\"      All other 117 states learn through value propagation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## Step 2: Train Q-Learning Agent\n",
    "\n",
    "We'll train a Q-learning agent with **optimized parameters** for sparse rewards.\n",
    "\n",
    "### Hyperparameters (OPTIMIZED for sparse rewards):\n",
    "- **Learning rate (α): 0.15** - Higher than typical (0.1) for faster learning\n",
    "- **Discount factor (γ): 0.95** - Higher than typical (0.9) for better value propagation\n",
    "- **Epsilon (ε): 0.02** - Very low for deterministic policy convergence\n",
    "- **Episodes: 300,000** - Much more than typical due to sparse rewards\n",
    "\n",
    "### Why so many episodes?\n",
    "With sparse rewards, positions far from the goal must learn through **value propagation**:\n",
    "- Position (4,5) learns quickly: it's adjacent to goal, gets direct +100 reward\n",
    "- Position (3,5) learns next: values propagate from (4,5)\n",
    "- Position (0,5) learns slowly: values must propagate through 5 intermediate states!\n",
    "\n",
    "This process requires many episodes. You'll see positions near the goal learn correctly first,\n",
    "while edge positions take much longer.\n",
    "\n",
    "**Estimated training time: 3-5 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    env=env,\n",
    "    learning_rate=0.15,      # Higher for faster learning with sparse rewards\n",
    "    discount_factor=0.95,    # Higher for better value propagation\n",
    "    epsilon=0.02             # Low for deterministic optimal policy\n",
    ")\n",
    "\n",
    "print(\"Training Q-learning agent with OPTIMIZED parameters...\")\n",
    "print(f\"  Learning rate: {agent.lr}\")\n",
    "print(f\"  Discount factor: {agent.gamma}\")\n",
    "print(f\"  Epsilon: {agent.epsilon}\")\n",
    "print(f\"  Episodes: 300,000\")\n",
    "print(f\"\\nThis will take a few minutes. Watch the scores increase!\\n\")\n",
    "\n",
    "scores = agent.train(num_episodes=300000, max_steps=50, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Note: Positions near the goal should have learned perfectly.\")\n",
    "print(\"      Some edge positions may still be improving.\")\n",
    "print(\"      For perfect convergence at ALL positions, try 500k+ episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Learning Convergence\n",
    "\n",
    "The learning curve shows how the agent's Q-values (summed across all states) increase over episodes.\n",
    "A rising curve indicates the agent is learning to estimate future rewards more accurately.\n",
    "\n",
    "You should see:\n",
    "- Initial rapid increase as positions adjacent to goal learn\n",
    "- Slower continued growth as values propagate to distant states\n",
    "- Eventually plateaus when most positions have converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning_curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(scores, window=1000, title=\"Q-Learning Convergence (11x11 Grid, 300k episodes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## Step 4: Value Function Heatmap\n",
    "\n",
    "The value function V(s) = max_a Q(s,a) shows the expected cumulative reward from each state.\n",
    "\n",
    "**What to look for**:\n",
    "- Brightest colors at the goal (highest value)\n",
    "- Values decrease as you move away from goal\n",
    "- Radial pattern centered on goal\n",
    "- Corners have lowest values (furthest from goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "value_heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_grid = agent.get_value_grid()\n",
    "plot_value_heatmap(value_grid, title=\"Learned State Values (11x11 Grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "## Step 5: Learned Policy Visualization\n",
    "\n",
    "**THIS IS THE KEY VISUALIZATION!**\n",
    "\n",
    "Arrows show the best action to take from each grid position.\n",
    "**All arrows should point TOWARD the center goal at (5,5)**.\n",
    "\n",
    "### What You Should See:\n",
    "- **Top half (rows 0-4)**: Arrows point DOWN ↓\n",
    "- **Bottom half (rows 6-10)**: Arrows point UP ↑\n",
    "- **Left side (cols 0-4)**: Arrows point RIGHT →\n",
    "- **Right side (cols 6-10)**: Arrows point LEFT ←\n",
    "- **Center row/column**: Arrows point directly to (5,5)\n",
    "\n",
    "### Visualization Details:\n",
    "- Plot uses `origin='upper'` (row 0 at top, like a normal matrix)\n",
    "- Action 0 (up) → arrow points upward\n",
    "- Action 1 (down) → arrow points downward\n",
    "- Red arrows show the learned policy\n",
    "\n",
    "### If Some Arrows Look Wrong:\n",
    "Those positions haven't fully converged yet. This is normal with sparse rewards!\n",
    "The positions closest to the goal should all be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "policy_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_grid = agent.get_policy_grid()\n",
    "plot_policy_arrows(policy_grid, value_grid, \n",
    "                  title=\"Learned Sensor Tasking Policy (11x11 Grid)\\nAll arrows should point toward goal at (5,5)\")\n",
    "\n",
    "# Print verification for key positions\n",
    "print(\"\\nPolicy Verification (Middle Column):\")\n",
    "print(\"=\"*50)\n",
    "action_names = {0: 'up ↑', 1: 'down ↓', 2: 'left ←', 3: 'right →'}\n",
    "for row in range(11):\n",
    "    action = policy_grid[row, 5]\n",
    "    if row < 5:\n",
    "        expected = \"down ↓ (to reach goal)\"\n",
    "        correct = \"✓\" if action == 1 else \"✗\"\n",
    "    elif row > 5:\n",
    "        expected = \"up ↑ (to reach goal)\"\n",
    "        correct = \"✓\" if action == 0 else \"✗\"\n",
    "    else:\n",
    "        expected = \"any (at goal)\"\n",
    "        correct = \"-\"\n",
    "    \n",
    "    print(f\"{correct} Row {row:2d}, Col 5: {action_names[action]:8s} | Expected: {expected}\")\n",
    "\n",
    "print(\"\\nIf you see ✗ marks, those positions need more training.\")\n",
    "print(\"Positions near the goal (rows 4-6) should always be ✓.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "## Step 6: Satellite Gimbal Visualization\n",
    "\n",
    "Now let's visualize how the learned grid policy translates to actual satellite gimbal pointing.\n",
    "\n",
    "We'll:\n",
    "1. Map grid positions to geographic coordinates (latitude/longitude)\n",
    "2. Convert to 3D Earth-Centered Earth-Fixed (ECEF) coordinates\n",
    "3. Show satellite gimbal pointing from orbit to ground targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gimbal_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satellite position (500 km altitude)\n",
    "satellite_pos = np.array([6371 + 500, 0, 0])  # Earth radius + altitude\n",
    "\n",
    "# Create multiple target positions from grid (using 11x11 grid)\n",
    "target_positions = []\n",
    "for gx in range(3, 9):  # Sample targets near center\n",
    "    for gy in range(3, 9):\n",
    "        tpos, _ = grid_to_satellite_coordinates(gx, gy, 11, 500.0)\n",
    "        target_positions.append(tpos)\n",
    "target_positions = np.array(target_positions)\n",
    "\n",
    "# Get pointing vector to center target\n",
    "_, gimbal_vec = grid_to_satellite_coordinates(5, 5, 11, 500.0)\n",
    "\n",
    "# Visualize\n",
    "create_satellite_gimbal_visualization(\n",
    "    satellite_position=satellite_pos,\n",
    "    target_positions=target_positions,\n",
    "    gimbal_pointing=gimbal_vec,\n",
    "    title=\"Satellite Gimbal Pointing to Learned Targets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. ✅ Created a Gymnasium environment for satellite sensor tasking\n",
    "2. ✅ Trained a Q-learning agent with optimized parameters for sparse rewards\n",
    "3. ✅ Visualized learning convergence and value functions\n",
    "4. ✅ Analyzed the learned policy with arrow visualizations\n",
    "5. ✅ Connected grid-based RL to physical satellite gimbal visualization\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### Sparse Rewards are Challenging!\n",
    "- Only 4 out of 121 states give direct rewards\n",
    "- Requires 300,000+ episodes for convergence\n",
    "- Positions far from goal learn through value propagation\n",
    "- This is realistic for many real-world RL problems!\n",
    "\n",
    "### Optimal Hyperparameters Matter\n",
    "- Higher learning rate (0.15) → faster learning\n",
    "- Higher discount (0.95) → better value propagation\n",
    "- Lower epsilon (0.02) → more deterministic policy\n",
    "\n",
    "## Next Steps for Students\n",
    "\n",
    "1. **Experiment with hyperparameters**: Try different learning rates, discount factors, epsilon values\n",
    "2. **Reduce episodes**: Train with only 50,000 episodes and see which positions fail to learn\n",
    "3. **Modify reward structure**: Add distance-based rewards to speed up learning\n",
    "4. **Multi-target scenarios**: Extend to multiple targets with different priorities\n",
    "5. **Different grid sizes**: Try 7×7 (easier) or 15×15 (harder)\n",
    "6. **Deep Q-Learning**: Replace Q-table with neural network for continuous states\n",
    "7. **Multi-satellite coordination**: Extend to multi-agent scenarios\n",
    "\n",
    "## References\n",
    "\n",
    "- Sutton & Barto, \"Reinforcement Learning: An Introduction\" (2018)\n",
    "- Gymnasium Documentation: https://gymnasium.farama.org/\n",
    "- Satellite RL Research: https://arxiv.org/html/2409.02270v1\n",
    "\n",
    "---\n",
    "\n",
    "🎉 **Notebook complete! You've successfully trained an RL agent for satellite sensor tasking!**\n",
    "\n",
    "**Note**: If policy arrows don't all point perfectly toward the goal, try training for 500,000 episodes or check the test images in `tests/` folder to see what correct arrows should look like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
