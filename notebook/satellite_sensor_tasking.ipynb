{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Sensor Tasking with Reinforcement Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand reinforcement learning fundamentals (states, actions, rewards, Q-learning)\n",
    "2. Apply RL to satellite sensor tasking problem\n",
    "3. Visualize agent learning and convergence\n",
    "4. Connect grid-based learning to physical satellite gimbal pointing\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "Satellites with sensors (cameras, radar) need to decide where to point to observe ground targets. \n",
    "This notebook models the problem as a grid where the agent learns to navigate to high-value target locations.\n",
    "\n",
    "**Setup**: Ensure you're using the satellite_rl kernel (see README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "from environment.grid_env import SatelliteSensorGridEnv\n",
    "from agents.q_learning import QLearningAgent\n",
    "from visualization.grid_viz import plot_learning_curve, plot_value_heatmap, plot_policy_arrows\n",
    "from visualization.satellite_viz import create_satellite_gimbal_visualization, grid_to_satellite_coordinates\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Matplotlib settings\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Environment\n",
    "\n",
    "We'll create an 11x11 grid environment where:\n",
    "- The satellite sensor can \"point\" to any of 121 positions\n",
    "- The goal (high-value target) is at the center position (state 60)\n",
    "- Rewards are given for reaching positions adjacent to the goal\n",
    "- Actions: 0=up, 1=down, 2=left, 3=right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SatelliteSensorGridEnv(grid_x=11, grid_y=11)\n",
    "\n",
    "print(f\"Environment created!\")\n",
    "print(f\"  Grid size: {env.grid_x}x{env.grid_y}\")\n",
    "print(f\"  Total states: {env.observation_space.n}\")\n",
    "print(f\"  Actions: {env.action_space.n} (up, down, left, right)\")\n",
    "print(f\"  Goal state: {env.goal_state}\")\n",
    "\n",
    "# Visualize initial random state\n",
    "state, _ = env.reset()\n",
    "print(f\"\\nInitial random state: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Q-Learning Agent\n",
    "\n",
    "We'll train a Q-learning agent for 30,000 episodes to learn the optimal sensor tasking policy.\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning rate (Î±): 0.1 - controls how much new information overrides old\n",
    "- Discount factor (Î³): 0.9 - importance of future rewards\n",
    "- Epsilon (Îµ): 0.1 - exploration rate (10% random actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    env=env,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.9,\n",
    "    epsilon=0.1\n",
    ")\n",
    "\n",
    "print(\"Training Q-learning agent...\")\n",
    "scores = agent.train(num_episodes=150000, max_steps=50, verbose=True)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Learning Convergence\n",
    "\n",
    "The learning curve shows how the agent's performance improves over episodes. \n",
    "A rising curve indicates the agent is learning to reach the goal more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(scores, window=500, title=\"Q-Learning Convergence (11x11 Grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Value Function Heatmap\n",
    "\n",
    "The value function V(s) shows the expected cumulative reward from each state. \n",
    "Brighter colors indicate higher values (closer to goal or better positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_grid = agent.get_value_grid()\n",
    "plot_value_heatmap(value_grid, title=\"Learned State Values (11x11 Grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Learned Policy Visualization\n",
    "\n",
    "Arrows show the best action to take from each grid position. \n",
    "All arrows should point toward the center (goal position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_grid = agent.get_policy_grid()\n",
    "plot_policy_arrows(policy_grid, value_grid, title=\"Learned Sensor Tasking Policy (11x11 Grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Satellite Gimbal Visualization\n",
    "\n",
    "Now let's visualize how the learned grid policy translates to actual satellite gimbal pointing.\n",
    "\n",
    "We'll:\n",
    "1. Map grid positions to geographic coordinates (latitude/longitude)\n",
    "2. Convert to 3D Earth-Centered Earth-Fixed (ECEF) coordinates\n",
    "3. Show satellite gimbal pointing from orbit to ground targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satellite position (500 km altitude)\n",
    "satellite_pos = np.array([6371 + 500, 0, 0])  # Earth radius + altitude\n",
    "\n",
    "# Create multiple target positions from grid (using 11x11 grid)\n",
    "target_positions = []\n",
    "for gx in range(3, 9):  # Sample targets near center\n",
    "    for gy in range(3, 9):\n",
    "        tpos, _ = grid_to_satellite_coordinates(gx, gy, 11, 500.0)\n",
    "        target_positions.append(tpos)\n",
    "target_positions = np.array(target_positions)\n",
    "\n",
    "# Get pointing vector to center target\n",
    "_, gimbal_vec = grid_to_satellite_coordinates(5, 5, 11, 500.0)\n",
    "\n",
    "# Visualize\n",
    "create_satellite_gimbal_visualization(\n",
    "    satellite_position=satellite_pos,\n",
    "    target_positions=target_positions,\n",
    "    gimbal_pointing=gimbal_vec,\n",
    "    title=\"Satellite Gimbal Pointing to Learned Targets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. âœ… Created a Gymnasium environment for satellite sensor tasking\n",
    "2. âœ… Trained a Q-learning agent to learn optimal sensor pointing\n",
    "3. âœ… Visualized learning convergence and value functions\n",
    "4. âœ… Connected grid-based RL to physical satellite gimbal visualization\n",
    "\n",
    "## Next Steps for Students\n",
    "\n",
    "1. **Experiment with hyperparameters**: Try different learning rates, discount factors\n",
    "2. **Modify reward structure**: Add penalties for distance traveled, rewards for coverage\n",
    "3. **Multi-target scenarios**: Extend to multiple targets with different priorities\n",
    "4. **Deep Q-Learning**: Implement neural network-based agent\n",
    "5. **Realistic orbits**: Integrate with poliastro for true orbital mechanics\n",
    "6. **Multi-satellite coordination**: Extend to multi-agent scenarios\n",
    "7. **Transfer learning**: Experiment with transferring knowledge to larger grids\n",
    "\n",
    "## References\n",
    "\n",
    "- Sutton & Barto, \"Reinforcement Learning: An Introduction\"\n",
    "- Gymnasium Documentation: https://gymnasium.farama.org/\n",
    "- Satellite RL Research: [arxiv.org/html/2409.02270v1](https://arxiv.org/html/2409.02270v1)\n",
    "\n",
    "ðŸŽ‰ **Notebook complete! Great work!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
