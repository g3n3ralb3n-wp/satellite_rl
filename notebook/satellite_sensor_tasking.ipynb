{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Satellite Sensor Tasking with Reinforcement Learning\n\n## Learning Objectives\n\n1. Understand reinforcement learning fundamentals (states, actions, rewards, Q-learning)\n2. Apply RL to satellite sensor tasking problem\n3. Visualize agent learning and convergence\n4. Connect grid-based learning to physical satellite gimbal pointing\n\n## Problem Overview\n\nSatellites with sensors (cameras, radar) need to decide where to point to observe ground targets. \nThis notebook models the problem as a grid where the agent learns to navigate to high-value target locations.\n\n**Setup**: Ensure you're using the satellite_rl kernel (see README.md)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nfrom pathlib import Path\n\n# Add src to path\nsys.path.append(str(Path('../src').resolve()))\n\nfrom environment.grid_env import SatelliteSensorGridEnv\nfrom agents.q_learning import QLearningAgent\nfrom visualization.grid_viz import plot_learning_curve, plot_value_heatmap, plot_policy_arrows\nfrom visualization.satellite_viz import create_satellite_gimbal_visualization, grid_to_satellite_coordinates\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Matplotlib settings\n%matplotlib inline\n\nprint(\"All imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Environment\n",
    "\n",
    "We'll create an 11x11 grid environment where:\n",
    "- The satellite sensor can \"point\" to any of 121 positions\n",
    "- The goal (high-value target) is at the center position (state 60)\n",
    "- Rewards are given for reaching positions adjacent to the goal\n",
    "- Actions: 0=up, 1=down, 2=left, 3=right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SatelliteSensorGridEnv(grid_x=11, grid_y=11)\n",
    "\n",
    "print(f\"Environment created!\")\n",
    "print(f\"  Grid size: {env.grid_x}x{env.grid_y}\")\n",
    "print(f\"  Total states: {env.observation_space.n}\")\n",
    "print(f\"  Actions: {env.action_space.n} (up, down, left, right)\")\n",
    "print(f\"  Goal state: {env.goal_state}\")\n",
    "\n",
    "# Visualize initial random state\n",
    "state, _ = env.reset()\n",
    "print(f\"\\nInitial random state: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Q-Learning Agent\n",
    "\n",
    "We'll train a Q-learning agent for 30,000 episodes to learn the optimal sensor tasking policy.\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning rate (Î±): 0.1 - controls how much new information overrides old\n",
    "- Discount factor (Î³): 0.9 - importance of future rewards\n",
    "- Epsilon (Îµ): 0.1 - exploration rate (10% random actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    env=env,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.9,\n",
    "    epsilon=0.1\n",
    ")\n",
    "\n",
    "print(\"Training Q-learning agent...\")\n",
    "scores = agent.train(num_episodes=30000, max_steps=50, verbose=True)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Learning Convergence\n",
    "\n",
    "The learning curve shows how the agent's performance improves over episodes. \n",
    "A rising curve indicates the agent is learning to reach the goal more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(scores, window=500, title=\"Q-Learning Convergence (11x11 Grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Value Function Heatmap\n",
    "\n",
    "The value function V(s) shows the expected cumulative reward from each state. \n",
    "Brighter colors indicate higher values (closer to goal or better positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_grid = agent.get_value_grid()\n",
    "plot_value_heatmap(value_grid, title=\"Learned State Values (11x11 Grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Learned Policy Visualization\n",
    "\n",
    "Arrows show the best action to take from each grid position. \n",
    "All arrows should point toward the center (goal position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_grid = agent.get_policy_grid()\n",
    "plot_policy_arrows(policy_grid, value_grid, title=\"Learned Sensor Tasking Policy (11x11 Grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 6: Satellite Gimbal Visualization\n\nNow let's visualize how the learned grid policy translates to actual satellite gimbal pointing.\n\nWe'll:\n1. Map grid positions to geographic coordinates (latitude/longitude)\n2. Convert to 3D Earth-Centered Earth-Fixed (ECEF) coordinates\n3. Show satellite gimbal pointing from orbit to ground targets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Satellite position (500 km altitude)\nsatellite_pos = np.array([6371 + 500, 0, 0])  # Earth radius + altitude\n\n# Create multiple target positions from grid (using 11x11 grid)\ntarget_positions = []\nfor gx in range(3, 9):  # Sample targets near center\n    for gy in range(3, 9):\n        tpos, _ = grid_to_satellite_coordinates(gx, gy, 11, 500.0)\n        target_positions.append(tpos)\ntarget_positions = np.array(target_positions)\n\n# Get pointing vector to center target\n_, gimbal_vec = grid_to_satellite_coordinates(5, 5, 11, 500.0)\n\n# Visualize\ncreate_satellite_gimbal_visualization(\n    satellite_position=satellite_pos,\n    target_positions=target_positions,\n    gimbal_pointing=gimbal_vec,\n    title=\"Satellite Gimbal Pointing to Learned Targets\"\n)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nIn this notebook, we:\n1. âœ… Created a Gymnasium environment for satellite sensor tasking\n2. âœ… Trained a Q-learning agent to learn optimal sensor pointing\n3. âœ… Visualized learning convergence and value functions\n4. âœ… Connected grid-based RL to physical satellite gimbal visualization\n\n## Next Steps for Students\n\n1. **Experiment with hyperparameters**: Try different learning rates, discount factors\n2. **Modify reward structure**: Add penalties for distance traveled, rewards for coverage\n3. **Multi-target scenarios**: Extend to multiple targets with different priorities\n4. **Deep Q-Learning**: Implement neural network-based agent\n5. **Realistic orbits**: Integrate with poliastro for true orbital mechanics\n6. **Multi-satellite coordination**: Extend to multi-agent scenarios\n7. **Transfer learning**: Experiment with transferring knowledge to larger grids\n\n## References\n\n- Sutton & Barto, \"Reinforcement Learning: An Introduction\"\n- Gymnasium Documentation: https://gymnasium.farama.org/\n- Satellite RL Research: [arxiv.org/html/2409.02270v1](https://arxiv.org/html/2409.02270v1)\n\nðŸŽ‰ **Notebook complete! Great work!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}